BI 111111 /////////////////////////////////////

!pip install Bio


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_table('sequence_2.txt', names=['sequence'])

sequence = df.sequence.iloc[1]

# GC Content
from Bio.Seq import Seq
from Bio.SeqUtils import gc_fraction

a_seq = Seq(sequence)
gc_content = gc_fraction(a_seq)
print("GC Content:", gc_content)

# MOTIF IDENTIFICATION
#regular expressions
import re
motif = "CTA"
q = re.compile(motif)
motif_indexes = [item.start(0) for item in q.finditer(sequence)]
print("Motif indexes:", motif_indexes)

# CODING region
def identify_code_reg(seq: str):
    coding_reg = []
    for i in range(len(seq)):
        if seq[i: i+3] == 'ATG':
            start_index = i

            for j in range(i+3, len(seq)-3):
                if seq[j: j+3] == "TAA" or seq[j: j+3] == "TGA" or seq[j: j+3] == "TAG":
                    stop_index = j
                    region = seq[i:j+3]
                    coding_reg.append( (i, j, region) )
                    break

    return coding_reg

cd = identify_code_reg(sequence)
print("Coding regions:", cd)

BI 222222 /////////////////////////////////////

!pip install scanpy

import pandas as pd
import numpy as np
import scanpy as sc

df = sc.read_csv('GSM5226574_C51ctr_raw_counts.csv')
df.X.shape

df.var['mt'] = df.var.index.str.startswith('MT-')




ribo_url = "http://software.broadinstitute.org/gsea/msigdb/download_geneset.jsp?geneSetName=KEGG_RIBOSOME&fileType=txt"
ribo_genes = pd.read_table(ribo_url, skiprows=2, header = None)
df.var['ribo'] = df.var_names.isin(ribo_genes[0].values)
sc.pp.calculate_qc_metrics(df, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True)

sc.pp.filter_genes(df, min_cells=3)

df.obs.columns

sc.pl.violin(df, ['n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt',
       'total_counts_ribo', 'pct_counts_ribo'], multi_panel=True, jitter=0.4)


upper_lim = np.quantile(df.obs.n_genes_by_counts, 0.98)
upper_lim

df = df[df.obs.n_genes_by_counts < upper_lim]
df = df[df.obs.pct_counts_mt < 20]
df = df[df.obs.pct_counts_ribo < 2]

sc.pp.normalize_total(df, target_sum=1e4)
sc.pp.log1p(df)
sc.pp.highly_variable_genes(df, n_top_genes=2000)
sc.pl.highly_variable_genes(df)

BI 444444 /////////////////////////////////////

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, auc

df5 = pd.read_csv('human_data1.csv')
df5.head()


X = df5['sequence']
y = df5['class']

le = LabelEncoder()
X = le.fit_transform(X)

X = X.reshape(-1, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

rfc = RandomForestClassifier()
rfc.fit(X_train, y_train)
rfc_acc = rfc.score(X_test, y_test)
print("Random Forest Accuracy:", rfc_acc)

svc = SVC()
svc.fit(X_train, y_train)
svc_acc = svc.score(X_test, y_test)
print("SVM Accuracy:", svc_acc)

accuracy_data = {'Model': ['RFC', 'SVC'], 'Accuracy': [rfc_acc, svc_acc]}
accuracy_df = pd.DataFrame(accuracy_data)
plt.figure(figsize=(8, 5))
sns.barplot(x='Model', y='Accuracy', data=accuracy_df)
plt.title('Accuracy Comparison between RFC and SVC')
plt.ylim(0, 1)
plt.show()

y_pred_rfc = rfc.predict(X_test)
y_pred_svc = svc.predict(X_test)
cm_rfc = confusion_matrix(y_test, y_pred_rfc)
cm_svc = confusion_matrix(y_test, y_pred_svc)

disp_rfc = ConfusionMatrixDisplay(confusion_matrix=cm_rfc)
disp_rfc.plot(cmap='Blues')
plt.title('RFC Confusion Matrix')
plt.show()

disp_svc = ConfusionMatrixDisplay(confusion_matrix=cm_svc)
disp_svc.plot(cmap='Blues')
plt.title('SVC Confusion Matrix')
plt.show()



IR 111111 /////////////////////////////////////

IR Assig 1
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = nltk.word_tokenize(text)
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)

def stem_text(text):
    stemmer = PorterStemmer()
    words = nltk.word_tokenize(text)
    stemmed_words = [stemmer.stem(word) for word in words]
    return ' '.join(stemmed_words)

def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    words = nltk.word_tokenize(text)
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(lemmatized_words)

def main():
    # Sample text
    text = "In a lively city, skyscrapers cast dynamic shadows, enticing exploration of local eateries. Tranquil parks offer an escape from urban life, blending modernity with history."

    # Remove stop words
    text_without_stopwords = remove_stopwords(text)
    print("Text after stop word removal:")
    print(text_without_stopwords)
    print()

    # Stemming
    stemmed_text = stem_text(text)
    print("Text after stemming:")
    print(stemmed_text)
    print()

    # Lemmatization (using NLTK)
    lemmatized_text = lemmatize_text(text)
    print("Text after lemmatization:")
    print(lemmatized_text)

if __name__ == "__main__":
    main()

IR 22222 /////////////////////////////////////

import string

document1 = "The dog barked at the sleeping cat"
document2 = "The lazy cat was sleeping under the shed."


def remove_p(doc):
  doc = doc = doc.translate(str.maketrans("", "", string.punctuation))
  return doc

document1 = remove_p(document1)
document2 = remove_p(document2)

tokens1 = document1.lower().split()
tokens2 = document2.lower().split()

terms = list(set(tokens1 + tokens2))

inverted_index = {}

for term in terms:
	documents = []
	if term in tokens1:
		documents.append(["Document 1", f"Index loc: {tokens1.index(term)}"])
	if term in tokens2:
		documents.append(["Document 2", f"Index loc: {tokens2.index(term)}"])
	inverted_index[term] = documents


key = input('Enter key to be searched: ').lower()

if key in inverted_index:
	print(key, "-", inverted_index[key])
else:
  print("Not present.")
print(terms)
print(inverted_index)

IR 33333 /////////////////////////////////////

! pip install pgmpy
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination

d3 = pd.read_csv('heart.csv')
print(d3.columns)

m = BayesianNetwork( [ ('age', 'target'), ('chol', 'target'), ('thalach', 'target'), ('target', 'restecg'), ('target', 'exang'),] )
m.fit(d3, estimator=MaximumLikelihoodEstimator)

infer = VariableElimination(m)

q = infer.query(variables=('target', 'exang'), evidence={'restecg': 1})
print(q)


IR 44444 /////////////////////////////////////

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, confusion_matrix , classification_report
import string
import seaborn as sns
import nltk

nltk.download('stopwords')

from nltk.corpus import stopwords

d4 = pd.read_csv('spam.csv')
d4.head()

d4.Message = d4.Message.str.lower()

def remove_punct(text):
  text_trans = str.maketrans('', '', string.punctuation)
  return text.translate(text_trans)
d4.Message = d4.Message.apply(remove_punct)



d4.Message = d4.Message.apply(remove_punct)
d4

string.punctuation

X = d4.Message
y = d4.Category

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y = le.fit_transform(y)

cv = CountVectorizer()
X = cv.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

m = MultinomialNB()
m.fit(X_train, y_train)

y_pred = m.predict(X_test)

accuracy_score(y_test, y_pred)


IR 55555 /////////////////////////////////////

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.metrics import silhouette_score
import scipy.cluster.hierarchy as shc

X = pd.read_csv('CC GENERAL')

X.head()

X = X.drop('CUST_ID', axis = 1)
X.fillna(method ='ffill', inplace = True)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_normalized = normalize(X_scaled)

X_normalized = pd.DataFrame(X_normalized)

pca = PCA(n_components = 2)
X_principal = pca.fit_transform(X_normalized)
X_principal = pd.DataFrame(X_principal)
X_principal.columns = ['P1', 'P2']

plt.figure(figsize =(8, 8))
plt.title('Visualising the data')
Dendrogram = shc.dendrogram((shc.linkage(X_principal, method ='ward')))

ac2 = AgglomerativeClustering(n_clusters = 2)

# Visualizing the clustering
plt.figure(figsize =(6, 6))
plt.scatter(X_principal['P1'], X_principal['P2'],
           c = ac2.fit_predict(X_principal), cmap ='rainbow')
plt.show()

ac4 = AgglomerativeClustering(n_clusters = 4)

plt.figure(figsize =(6, 6))
plt.scatter(X_principal['P1'], X_principal['P2'],
            c = ac4.fit_predict(X_principal), cmap ='rainbow')
plt.show()

ac6 = AgglomerativeClustering(n_clusters = 6)

plt.figure(figsize =(6, 6))
plt.scatter(X_principal['P1'], X_principal['P2'],
            c = ac6.fit_predict(X_principal), cmap ='rainbow')
plt.show()

k = [2, 4, 6]

silhouette_scores = []

silhouette_scores.append(
        silhouette_score(X_principal, ac2.fit_predict(X_principal)))

silhouette_scores.append(
        silhouette_score(X_principal, ac4.fit_predict(X_principal)))

silhouette_scores.append(
        silhouette_score(X_principal, ac6.fit_predict(X_principal)))

plt.bar(k, silhouette_scores)
plt.xlabel('Number of clusters', fontsize = 20)
plt.ylabel('S(i)', fontsize = 20)
plt.show()

IR 66666 /////////////////////////////////////
import numpy as np


def calculate_pagerank(adjacency_matrix, damping_factor=0.85, max_iterations=100, tolerance=1e-6):
    """
    Calculate PageRank using the power iteration method.

    Parameters:
        adjacency_matrix (numpy.ndarray): Adjacency matrix representing the graph.
        damping_factor (float): Damping factor for the PageRank algorithm.
        max_iterations (int): Maximum number of iterations for convergence.
        tolerance (float): Convergence tolerance.

    Returns:
        numpy.ndarray: PageRank scores for each node.
    """
    num_nodes = len(adjacency_matrix)
    initial_pagerank = np.ones(num_nodes) / num_nodes
    pagerank = initial_pagerank

    for i in range(max_iterations):
        new_pagerank = (1 - damping_factor) / num_nodes + damping_factor * np.dot(adjacency_matrix.T, pagerank)
        if np.linalg.norm(new_pagerank - pagerank) < tolerance:
            break
        pagerank = new_pagerank

    return pagerank
